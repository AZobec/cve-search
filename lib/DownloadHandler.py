import multiprocessing as mp
import os
import threading
import time
from abc import ABC, abstractmethod
from concurrent.futures.thread import ThreadPoolExecutor
from datetime import timedelta
from itertools import islice

import requests

from lib.Config import Configuration as conf

thread_local = threading.local()


class DownloadHandler(ABC):
    """
    DownloadHandler is the base class for all downloads and subsequent processing of the downloaded content.
    Each download script has a derived class which handles specifics for that type of content / download.
    """

    def __init__(self):
        self._end = None

    def __repr__(self):
        """ return string representation of object """
        return "<< DownloadHandler:{} >>"

    def get_session(self):
        """
        Method for returning a session object per every requesting thread
        """
        if not hasattr(thread_local, "session"):
            thread_local.session = requests.Session()
        return thread_local.session

    def process_downloads(self, sites, q, collection):
        """


        :param sites:
        :type sites:
        :param q:
        :type q:
        :param collection:
        :type collection:
        :return:
        :rtype:
        """

        worker_size = min(32, os.cpu_count() + 4)

        start_time = time.time()
        with ThreadPoolExecutor() as executor:
            executor.map(self.download_site, sites)

        print("Processing finished!")

        print("Saving queue to database!")

        self._process_queue_to_db(worker_size, q, collection=collection)

        print("Duration: {}".format(timedelta(seconds=time.time() - start_time)))

    def chunk_list(self, lst, number):
        """
        Yield successive n-sized chunks from lst.

        :param lst: List to be chunked
        :type lst: list
        :param number: Chunk size
        :type number: int
        :return: Chunked list
        :rtype: list
        """
        for i in range(0, len(lst), number):
            yield lst[i: i + number]

    def process_chunks_in_function(self, function, chunks, max_workers=None):
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            executor.map(function, chunks)

    def _process_queue_to_db(self, max_workers, q, collection):
        """


        :param max_workers:
        :type max_workers:
        :param q:
        :type q:
        :param collection:
        :type collection:
        :return:
        :rtype:
        """

        processes = [mp.Process(target=self._db_bulk_writer, args=(q, collection,))
                     for i in range(max_workers)]
        for proc in processes:
            proc.start()
            # Put SENTINELs in the Queue to tell the workers to exit their for-loop
            q.put(self._end)
        for proc in processes:
            proc.join()

    def _db_bulk_writer(self, output_queue, collection, threshold=1000):
        """


        :param output_queue:
        :type output_queue:
        :param collection:
        :type collection:
        :param threshold:
        :type threshold:
        :return:
        :rtype:
        """
        db = conf.getMongoConnection()
        items = iter(output_queue.get, self._end)
        for batch in iter(lambda: list(islice(items, threshold)), []):
            db[collection].bulk_write(batch)

    @abstractmethod
    def download_site(self, **kwargs):
        pass

    @abstractmethod
    def stream_json(self, **kwargs):
        pass

    @abstractmethod
    def update(self, **kwargs):
        pass

    @abstractmethod
    def populate(self, **kwargs):
        pass
