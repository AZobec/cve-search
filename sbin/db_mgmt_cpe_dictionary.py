#!/usr/bin/env python3
#
# Import script of nvd cpe (Common Platform Enumeration) definition
# into a collection used for human readable lookup of product name.
#
# Imported in cvedb in the collection named cpe.
#
# The format of the collection is the following
#
# { "_id" : ObjectId("50a2739eae24ac2274eae7c0"), "id" :
# "cpe:/a:1024cms:1024_cms:0.7", "title" : "1024cms.org 1024 CMS 0.7" }
#
# Software is free software released under the "GNU Affero General Public License v3.0"
#
# Copyright (c) 2012 		Wim Remes
# Copyright (c) 2012-2018  Alexandre Dulaunoy - a@foo.be
# Copyright (c) 2014-2018  Pieter-Jan Moreels - pieterjan.moreels@gmail.com

# Imports
import argparse
import hashlib
import json
import logging
import os
import shutil
import sys
import time
import zipfile
from io import BytesIO
from itertools import islice
from multiprocessing import Queue

import ijson
from pymongo import UpdateOne
from dateutil.parser import parse as parse_datetime
from tqdm import tqdm

runPath = os.path.dirname(os.path.realpath(__file__))
sys.path.append(os.path.join(runPath, ".."))

from lib.Config import Configuration as conf
from lib.DownloadHandler import JSONFileHandler
from lib.Config import Configuration
import lib.DatabaseLayer as db
from lib.Toolkit import generate_title

# parse command line arguments
argparser = argparse.ArgumentParser(
    description="populate/update the local CPE database"
)
argparser.add_argument("-u", action="store_true", help="update the database")
argparser.add_argument("-p", action="store_true", help="populate the database")
argparser.add_argument(
    "-a", action="store_true", default=False, help="force populating the CPE database"
)
argparser.add_argument(
    "-f", action="store_true", default=False, help="force update of the CPE database"
)
argparser.add_argument("-v", action="store_true", help="verbose output")
args = argparser.parse_args()


def process_cpe_item(item=None):
    if item is None:
        return None
    if "cpe23Uri" not in item:
        return None

    cpe = {
        "title": generate_title(item["cpe23Uri"]),
        "cpe_2_2": item["cpe23Uri"],
        "cpe_name": item["cpe_name"],
    }
    version_info = ""
    if "versionStartExcluding" in item:
        cpe["versionStartExcluding"] = item["versionStartExcluding"]
        version_info += cpe["versionStartExcluding"]
    if "versionStartIncluding" in item:
        cpe["versionStartIncluding"] = item["versionStartIncluding"]
        version_info += cpe["versionStartIncluding"]
    if "versionEndExcluding" in item:
        cpe["versionEndExcluding"] = item["versionEndExcluding"]
        version_info += cpe["versionEndExcluding"]
    if "versionEndIncluding" in item:
        cpe["versionEndIncluding"] = item["versionEndIncluding"]
        version_info += cpe["versionEndIncluding"]

    sha1_hash = hashlib.sha1(
        cpe["cpe_2_2"].encode("utf-8") + version_info.encode("utf-8")
    ).hexdigest()
    cpe["id"] = sha1_hash

    return cpe


class CPEDownloads(JSONFileHandler):
    def __init__(self):
        self.feed_type = "CPE"
        super().__init__(self.feed_type, multi_proc_q=False)

        self.feed_url = Configuration.getFeedURL(self.feed_type.lower())

        self.queue = Queue()

        self.logger = logging.getLogger("CPEDownloads")

    def download_site(self, url):
        self.logger.debug("Downloading: {}".format(url))
        session = self.get_session()
        with session.get(url) as response:
            self.last_modified = parse_datetime(
                response.headers["last-modified"], ignoretz=True
            )

            self.logger.debug(
                "Last {} modified value: {}".format(self.feed_type, self.last_modified)
            )

            i = db.getInfo(self.feed_type.lower())

            if i is not None:
                if self.last_modified == i["last-modified"]:
                    self.logger.debug(
                        "{}'s are not modified since the last update".format(
                            self.feed_type
                        )
                    )
                    return

            fzip = zipfile.ZipFile(BytesIO(response.content), "r")
            if len(fzip.namelist()) > 0:
                data = BytesIO(fzip.read(fzip.namelist()[0]))

            cpej = json.loads(data.read())

            cpe_list = [
                UpdateOne(
                    {"id": process_cpe_item(cpe)["id"]},
                    {"$set": process_cpe_item(cpe)},
                    upsert=True,
                )
                for cpe in cpej["matches"]
            ]

            self.logger.debug("Processed {} entries for: {}".format(len(cpe_list), self.feed_type))

            self.queue.put(cpe_list)

            # self._db_bulk_writer(cpe_list, "cpe")

            # db.bulkUpdate("cpe", cpe_gen, len(cpej["matches"]))

            # content_type = response.headers["content-type"]
            #
            # wd, filename = self.store_file(
            #     response_content=response.content, content_type=content_type, url=url
            # )
            #
            # if filename is not None:
            #     self.file_to_queue(working_dir=wd, filename=filename)
            # else:
            #     self.logger.error(
            #         "Unable to retrieve a filename; something went wrong during the storing of the file"
            #     )

    def _db_bulk_writer(self, output_queue, collection, threshold=1000):
        """
        Method to act as worker for writing queued entries into the database

        :param output_queue: Multiprocessing Queue reference
        :type output_queue: Queue
        :param collection: Mongodb Collection name
        :type collection: str
        :param threshold: Batch size threshold; defaults to 1000
        :type threshold: int
        """
        db = conf.getMongoConnection()
        items = output_queue.get()

        proc_bar = tqdm(total=len(items), desc="Writing entries to database")
        for batch in self.chunk_list(items, threshold):
            db[collection].bulk_write(batch)
            proc_bar.update(len(batch))

    def file_to_queue(self, working_dir, filename):

        with open(filename, "rb") as input_file:
            x = 0
            for cpe in ijson.items(input_file, "matches.item"):
                self.queue.put(self.process_cpes(cpeitem=cpe))
                x += 1

            self.logger.debug("Processed {} entries from file: {}".format(x, filename))

        try:
            self.logger.debug("Removing working dir: {}".format(working_dir))
            shutil.rmtree(working_dir)
        except Exception as err:
            self.logger.error(
                "Failed to remove working dir; error produced: {}".format(err)
            )

    def process_cpes(self, cpeitem):
        cpe = process_cpe_item(cpeitem)
        return UpdateOne({"id": cpe["id"]}, {"$set": cpe}, upsert=True,)

    def update(self, **kwargs):
        self.logger.info("CPE database update started")

        self.process_downloads(
            [self.feed_url], self.queue, collection=self.feed_type.lower(),
        )

        self.logger.info("Finished CPE database update")

        return self.last_modified

    def populate(self, **kwargs):
        self.logger.info("CPE Database population started")
        db.dropCollection(self.feed_type.lower())

        self.process_downloads(
            [self.feed_url], self.queue, collection=self.feed_type.lower(),
        )

        self.logger.info("Finished CPE database population")

        return self.last_modified


if __name__ == "__main__":
    cpd = CPEDownloads()
    cpd.logger.debug("{}".format(" ".join(sys.argv)))

    if args.u:

        last_modified = cpd.update()

    elif args.p:
        c = db.getSize(cpd.feed_type.lower())
        if args.v:
            cpd.logger.info(str(c))
        if c > 0 and args.a is False:
            cpd.logger.info("Database already populated")
        else:
            last_modified = cpd.populate()
